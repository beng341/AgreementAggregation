#!/usr/bin/env python3# -*- coding: utf-8 -*-"""Created on Thu Oct 12 16:38:40 2023@author: eminberker"""# True ranking is 1>2>3>4>...>mimport numpy as npimport randomimport choiximport pandas as pdimport mathimport timeimport networkx as nximport sdopt_tearing_master.grb_lazy as glimport sdopt_tearing_master.grb_pcm as gpcmfrom tqdm import tqdmfrom scipy.optimize import minimize, LinearConstraintfrom scipy.special import logsumexp, expituse_gurobi = Trueif use_gurobi:    import gurobipy as gp    from gurobipy import GRBimport torch# -------------------------------------------------------------# HELPER FUNCTIONS AFTER REVIEWS ARE GENERATED:def kendall_tau_ranking(weights, ranking1, ranking2,                        n):  # computes wegihted kendall tau distance between two input rankings    positions1 = np.zeros(n)    positions2 = np.zeros(n)    for i in range(len(ranking1)):        for j in ranking1[i]:            positions1[j] = i    for i in range(len(ranking2)):        for j in ranking2[i]:            positions2[j] = i    dist = 0    for i in range(n):        for j in range(i + 1, n):            if positions1[i] > positions1[j] and positions2[i] < positions2[j]:                dist += weights[i] * weights[j]            elif positions1[i] < positions1[j] and positions2[i] > positions2[j]:                dist += weights[i] * weights[j]            elif positions1[i] == positions1[j] or positions2[i] == positions2[j]:                dist += weights[i] * weights[j] / 2    return distdef topological_sort_kahn(adj_matrix):  # topologically sorts a DAG, ChatGPT generated.    n = adj_matrix.shape[0]    in_degree = np.count_nonzero(adj_matrix, axis=0)    queue = [i for i in range(n) if in_degree[i] == 0]    top_order = []    while queue:        node = queue.pop(0)        top_order.append(node)        for i in range(n):            if adj_matrix[node, i] > 0:                in_degree[i] -= 1                if in_degree[i] == 0:                    queue.append(i)    if len(top_order) != n:        return None  # Cycle detected or graph is not a DAG    return top_orderdef ave_ranking_scores(args, proposal_ranks, max_mix_rejection, rms_rejection):    (n, m, k, l) = args    b = proposal_ranks.copy()    ave_ratings = [0 for _ in range(m)]  # average score for each rankings    for i in range(m):        b[i] = proposal_ranks[i].copy()        if max_mix_rejection and len(b[i]) != 0:            max_value = max(b[i])            min_value = min(b[i])            # Remove the max and min elements from the list            # Note: If there are duplicates of the max/min values, this will remove only the first occurrence of each            b[i].remove(max_value)            if len(b[i]) != 0:                b[i].remove(min_value)        if rms_rejection and len(b[i]) != 0:            filtered_list, rms = compute_rms(b[i], z=3)            b[i] = filtered_list        if len(b[i]) == 0:            ave_ratings[i] = -1  # (something smaller than all others)        else:            ave_ratings[i] = max(max(proposal_ranks)) - sum(b[i]) / len(b[i])    return ave_ratingsdef compute_rms(my_list, z=3):    # Step 1: Calculate the Mean    mean_value = sum(my_list) / len(my_list)    # Step 2: Calculate the RMS    rms = math.sqrt(sum(x ** 2 for x in my_list) / len(my_list))    # Step 3: Identify and Remove Outliers    filtered_list = [x for x in my_list if abs(x - mean_value) <= z * rms]    return filtered_list, rmsdef reviewer_split(reviews, args, gamma):  # Splits the reviewer set into two and returns a M^(k) matrix for each of k in {1,2}, where M^(k)_{ij} records how many reviewers ranked proposal i in position j    """    Splits set of voters into two groups.    Creates a ranking matrix for each different split.    :param reviews:    :param args:    :param gamma:    :return:    """    (n, m, k, l) = args    all_reviewers = [i for i in range(n)]    reviewers1 = set(random.sample(all_reviewers, int(n / 2)))    reviewers2 = set(all_reviewers) - reviewers1    M1 = torch.Tensor([[0] * m for _ in range(k)])  # M1_jp = how many reviewers in set1 ranked prporsal p on position j    M2 = torch.Tensor([[0] * m for _ in range(k)])  # M2_jp = how many reviewers in set2 ranked prporsal p on position j    M_sum = torch.Tensor([[0] * m for _ in range(k)])  # in total    proposal_split = np.zeros((m, 2))    reviewers = [reviewers1, reviewers2]    M = [M1, M2, M_sum]    for i in [0, 1]:        for r in reviewers[i]:            for j in range(len(reviews[r])):                M[i][j][reviews[r][j]] += 1                M[2][j][reviews[r][j]] += 1                proposal_split[reviews[r][j]][i] += 1        total_reviewers = torch.sum(M[i], dim=0)        mask = total_reviewers != 0        M[i][:, mask] = M[i][:, mask] / total_reviewers[mask]    total_reviewers = torch.sum(M[2], dim=0)    mask = total_reviewers != 0    M[2][:, mask] = M[2][:, mask] / total_reviewers[mask]    proposal_split = np.min(proposal_split, 1)    weights = ((gamma ** proposal_split) - 1) / (gamma ** (int(l / 2)) - 1)    return reviewers, M, weightsdef total_scores(M, a, args):    """    returns the total scores associated with a given split and weights.    :M: list of integers of size (l x m), M_ip indicates how many reviewers have ranked proposal p on position i    :a: list of variables of size (1 x l), the positional weights (1=a[0]>a[1]>...>a[l-1]=0)    :return: the total scores associated with this split    """    (n, m, k, l) = args    M1 = M[0]    M2 = M[1]    total_score_1 = [sum(a[i] * M1[:, p][i] for i in range(k)) for p in range(m)]    total_score_2 = [sum(a[i] * M2[:, p][i] for i in range(k)) for p in range(m)]    return total_score_1, total_score_2def total_scores_all(M, a, args):    """    returns the total scores when the entire reviewer set is treated as one.    :M: list of integers of size (l x m), M_ip indicates how many reviewers have ranked proposal p on position i    :a: list of variables of size (1 x l), the positional weights (1=a[0]>a[1]>...>a[l-1]=0)    :return: the total scores associated with this split    """    (n, m, k, l) = args    M_sum = M[2]    total_score = [sum(a[i] * M_sum[:, p][i] for i in range(k)) for p in range(m)]    return total_scoredef kendall_tau(M, weights, a, args,                nabla):  # computes the weighted kendall-tau disagreement between two sets according to the positional scoring with the scores given in input a    (n, m, k, l) = args    error = 0    total_score_1, total_score_2 = total_scores(M, a, args)    disagreements = []    ties = []    for i in range(m):        for j in range(i + 1, m):            # if i==1 and j==4:             # print(total_score_1[i]==total_score_1[j])            if (total_score_1[i] > total_score_1[j] + nabla and total_score_2[i] + nabla < total_score_2[j]) or (                    total_score_1[i] + nabla < total_score_1[j] and total_score_2[i] > total_score_2[j] + nabla):                # print("error", i,j)                disagreements.append((i, j))                error += weights[i] * weights[j]            elif abs(total_score_1[i] - total_score_1[j]) < nabla or abs(total_score_2[i] - total_score_2[j]) < nabla:                # print("half error", i,j)                ties.append((i, j))                error += weights[i] * weights[j] / 2    return error, disagreements, tiesdef kendall_tau_scores(weights, total_score_1, total_score_2, args, nabla=1e-6, punish_ties=True):    (n, m, k, l) = args    error = 0    disagreements = []    ties = []    for i in range(m):        for j in range(i + 1, m):            # if i==1 and j==4:             # print(total_score_1[i]==total_score_1[j])            if (total_score_1[i] > total_score_1[j] + nabla and total_score_2[i] + nabla < total_score_2[j]) or (                    total_score_1[i] + nabla < total_score_1[j] and total_score_2[i] > total_score_2[j] + nabla):                # print("error", i,j)                disagreements.append((i, j))                error += weights[i] * weights[j]            elif abs(total_score_1[i] - total_score_1[j]) < nabla or abs(total_score_2[i] - total_score_2[j]) < nabla:                # print("half error", i,j)                if punish_ties or not (abs(total_score_1[i] - total_score_1[j]) < nabla and abs(                        total_score_2[i] - total_score_2[                            j]) < nabla):  # if punish_ties=True, then one ranking being a tie is sufficent for punishement. otherwise, we do not punish if both rankings are a tie                    ties.append((i, j))                    error += weights[i] * weights[j] / 2    return error, disagreements, tiesdef build_pairwise_graph(reviews, args):  # given reviews (pytorch tensor), generates the pairwise comparison matrix    # Initialize tensor B with zeros    (n, m, k, l) = args    B = torch.zeros(m, m, dtype=torch.int32)    for row in reviews:        for i, first_num in enumerate(row):            for second_num in row[i + 1:]:                B[first_num, second_num] += 1                B[second_num, first_num] -= 1    return torch.maximum(B, torch.tensor(0.))# -------------------------------------------------------------# (VARIOUS FUNCTIONS) FOR LEARNING THE BEST RULE, GIVEN THE REVIEWER SPLIT:def position_scoring(M, weights, args, steep=300):    def objective_func_helper(a_in):        # Calculate the total scores based on current points        full_a = np.concatenate(([1], a_in))        total_score_1, total_score_2 = total_scores(M, full_a, args)        # initialize matrix to host the errors        d = [[0] * m for _ in range(m)]        # calculate pairwise errors between the two splits        for i in range(m):            for j in range(i + 1, m):                d[i][j] = weights[i] * weights[j] * expit(                    -(total_score_1[i] - total_score_1[j]) * (total_score_2[i] - total_score_2[j]) * steep)        return total_score_1, total_score_2, d    def objective_func(a_in):        # get helper values        total_score_1, total_score_2, d = objective_func_helper(a_in)        # return the objective function, which is the total pairwise errors        return sum([d[i][j] for i in range(m) for j in range(i + 1, m)])    def get_constraint_matrix(score_count):        # matrix representing 1 >= a_1 >= a_2 ... >= a_k >= 0         A = np.zeros((score_count + 1, score_count))        A[0][0] = 1  # first row represents only a_1        A[score_count][score_count - 1] = 1  # last row represents only a_k        for i in range(1, score_count):            A[i][i - 1] = 1            A[i][i] = -1        return A    # get arguments    (n, m, k, l) = args    # initiate uniformly distributed points to be given to each candidate rank    # size is k - 1 since a[0]=1, so we won't pass it into the optimizer    a_initial_values = np.linspace(1.0, 0.0, num=(k - 1))    a_length = len(a_initial_values)    lb = np.zeros((a_length + 1,))    ub = np.ones((a_length + 1,))    # use COBYLA optimizer to get best scores for a_k ... a_1    res = minimize(objective_func, a_initial_values, method='COBYLA',                   constraints=LinearConstraint(get_constraint_matrix(a_length), lb=lb, ub=ub), options={'disp': True})    best_scores = res.x    # add a_0 = 1 to the resulting array    optimized_scores = np.concatenate(([1], best_scores))    # scipy sometimes output extremely small negative numbers(e.g "-8.28102464e-21") for a_k and violate a_k > 0 constraint, so we enforce it    if optimized_scores[-1] < 0:        optimized_scores[-1] = 0    return optimized_scoresdef loss_sigmoid_torch(M, weights, args, nabla, steep, sensitivity, min_steps, max_steps, lr=1e-1, print_mode=True,                       opt_type="Adam", time_out=None):    # Example function: a simple sigmoid    (n, m, k, l) = args    try:        losses = []        # a_initial_values = torch.cat((torch.ones(k // 2), torch.zeros(k-(k//2))))        a_initial_values = torch.linspace(1.0, 0.0, steps=k)        a = [torch.tensor([val], requires_grad=True) for val in a_initial_values]  # check different initializiations        #     z.setObjective( sum( sig[i][j] for i in range(m) for j in range(i+1,m)), GRB.MINIMIZE)        #     z.params.NonConvex=2        #     z.write("out.lp")        #     z.optimize()        #     for v in a[1:-1]:        #         print('%s %g' % (v.VarName, v.X), end="; ")        #     # for v in z.getVars():        #     #     print('%s %g' % (v.VarName, v.X), end="; ")        #     print('Obj: %g' % z.ObjVal)        # Optimizer: here we use basic gradient descent with a learning rate of 1e-1        if opt_type == "Adam":            optimizer = torch.optim.Adam([a[i] for i in range(1, k - 1)], lr=lr)        elif opt_type == "SGD":            optimizer = torch.optim.SGD([a[i] for i in range(1, k - 1)], lr=lr)        # Number of steps we want to take        # Gradient descent loop        torch.autograd.set_detect_anomaly(False)        step = 0        old_loss = 100001        new_loss = 100000        good_runs = 0        n_plat = 0        a_values = [ai.item() for ai in a]        if print_mode:            print(f"Step {step}: a = {a_values}")        start = time.time()        while (lr > 1e-6) and (n_plat < 100 or step < min_steps) and step < max_steps and (                time_out is None or time.time() - start < time_out):            if old_loss - new_loss <= sensitivity:  # if the loss is not decreasing for too many steps, quit                n_plat += 1            else:                n_plat = 0            if new_loss > old_loss + 0.1 / (step // 100 + 1):                lr = lr / 10                for param_group in optimizer.param_groups:                    param_group['lr'] = lr                if print_mode:                    print("NEW LEARNING RATE:", lr)                good_runs = 0            elif new_loss < old_loss:                good_runs += 1                if good_runs > 100:                    lr = lr * 10                    for param_group in optimizer.param_groups:                        param_group['lr'] = lr                    if print_mode:                        print("NEW LEARNING RATE:", lr)                    good_runs = 0            else:                good_runs = 0            # Zero the gradients from the previous step            old_loss = new_loss            optimizer.zero_grad()            # Calculate the function value            total_score_1, total_score_2 = total_scores(M, a, args)            d = [[0] * m for _ in range(m)]  # to host the errors            for i in range(m):                for j in range(i + 1, m):                    d[i][j] = weights[i] * weights[j] * torch.special.expit(                        -(total_score_1[i] - total_score_1[j]) * (total_score_2[i] - total_score_2[j]) * steep)            y = sum([d[i][j] for i in range(m) for j in range(i + 1, m)])            if step == 0:                y_best = y                a_best = [ai.item() for ai in a]            if y < y_best:                y_best = torch.clone(y)                a_best = [ai.item() for ai in a]            # Compute gradients of the function with respect to x            y.backward()            # Take a step in the direction of the negative gradient            a_values = [ai.item() for ai in a]            # if step%10==0:            #     print(f"Pre-Step {step}: a = {a_values}, sum of sigmoids = {y.item()}")            # for i in range(20):            #     print("gradient of a",i,"is", a[i].grad)            # for i in range(4):            #     torch.nn.utils.clip_grad_norm_(a[i], 10000)            # if step%10==0:            #     print(f"Post-clip -Step {step}: a = {a_values}, sum of sigmoids = {y.item()}")            # for i in range(4):            #     print("gradient of a",i,"is", a[i].grad)            optimizer.step()            # Print the current value of x and the function value            # Project the variables to satisfy the constraints a[i] <= a[i-1] for all i            with torch.no_grad():                for i in range(1, k - 1):                    a[i].data = torch.minimum(a[i], a[i - 1])                    a[i].data = torch.maximum(a[i], a[k - 1])            # Print the current value of a and the function value            a_values = [ai.item() for ai in a]            if step % 10 == 0 and print_mode:                print(f"Step {step}: sum of sigmoids = {y.item()}")                print(f"Step {step}: a = {a_values}, sum of sigmoids = {y.item()}")                # for i in range(k):                #     print("gradient of a[",i,"] is: ",a[i].grad)            step += 1            new_loss = y.item()            losses.append(new_loss)        return a_best, losses        #        # The variable x should now be at the minimum of the sigmoid function        # which will be close to the point where the derivative is zero.    except AttributeError:        print('Encountered an attribute error')def kemeny_gurobi_lazy(reviews, args, time_out=None, printout_mode=False):    """Kemeny-Young optimal rank aggregation"""    (n, m, k, l) = args    # maximize c.T * x    edge_weights = build_pairwise_graph(reviews, args)    edge_weights_np = edge_weights.numpy()    G = nx.DiGraph()    for i in range(m):        for j in range(m):            if edge_weights_np[i, j] != 0:  # assuming 0 means no edge                G.add_edge(i, j, weight=edge_weights_np[i, j])    G = gpcm.add_orig_edges_map(G)    G = nx.DiGraph(G)    elims, cost, cycle_matrix = gl.solve_problem(G, time_out=time_out, print_mode=printout_mode)    for (u, v) in elims:        edge_weights_np[u, v] = 0.0    ranking = topological_sort_kahn(edge_weights_np)    # print(ranking)    # scores=np.argsort(ranking)    # print(scores)    # for i in range(m):    #     for j in range(i+1,m):    #         if edge_weights_np[i,j]>0:    #             assert scores[i] < scores[j]     return ranking, costdef print_from_scores(ranking_scores, method_name,                      file_writer):  # Given the scores of the ranking, prints the ranking itself, including with ties    ranking_indices = np.flip(np.argsort(ranking_scores))    ranking_output = [list(dict_id.keys())[list(dict_id.values()).index(val)] for val in ranking_indices]    print("Output with " + method_name + ":")    file_writer.write("Output with " + method_name + ":" + "\n")    current_score = ranking_scores[ranking_indices[0]]    # print("here we go", current_score)    current_line = "1: "    for i in range(len(ranking_indices)):        if ranking_scores[ranking_indices[i]] == current_score:            if i > 0:                current_line += ", "            current_line += ranking_output[i]        else:            if isinstance(current_score, torch.Tensor):                current_line += " (Score: " + str(current_score.item()) + ")"            else:                current_line += " (Score: " + str(current_score) + ")"            print(current_line)            file_writer.write(current_line + "\n")            current_line = str(i + 1) + ": " + ranking_output[i]            current_score = ranking_scores[ranking_indices[i]]    if isinstance(current_score, torch.Tensor):        current_line += " (Score: " + str(current_score.item()) + ")"    else:        current_line += " (Score: " + str(current_score) + ")"    print(current_line)    file_writer.write(current_line + "\n")    print()    file_writer.write("\n")# In[ ]:run_tests = Falseif run_tests:    # Step 1: Import CSV file    file_path = "XXX"    data = pd.read_csv(file_path)    # Step 2: Count distinct values in column 1 and 2    m = data['Submission id'].nunique()  # number of proposals    n = data['Reviewer id'].nunique()  # number of reviewers    l = 10  # number of proposals per reviewer    k = 10    reviews = [[None for _ in range(l)] for _ in range(n)]  # creating an empty reviews table, to be filled    print("args:", n, m, l, k)    # In[ ]:    # Step 3: Create dictionaries for columns 1 and 2    dict_id = {value: index for index, value in enumerate(data['Reviewer id'].unique())}    for i in range(len(data)):        id_proposal = dict_id[data['Submission id'][i]]        id_reviewer = dict_id[data['Reviewer id'][i]]        rank = data['rank'][i]        if np.isnan(rank):            continue        # print(rank)        reviews[id_reviewer][int(rank) - 1] = id_proposal    for i in range(len(reviews)):  # remove None cells        old = reviews[i]        reviews[i] = [value for value in old if value is not None]    # In[ ]:    args = (n, m, k, l)    gamma = 2    reviewers, M, weights = reviewer_split(reviews, args, gamma)    max_kt = 0  # The maximum weighted KT possible for this specific split    for i in range(n):        for j in range(i + 1, n):            max_kt += weights[i] * weights[j]    min_steps = 100    max_steps = 5000    steep = 300    sensitivity = 0.001    lr = 1e-1    nabla = 0.001    step = 5400    time_out = 6 * 60 * 60    pr = 0.75  # probability of a reviwer getting the right ranking when comparing two    with open('results_10.txt', 'w') as p:        print("Timeout limit used for Kemeny and Torch: " + str(time_out) + "s")        p.write("Timeout limit used for Kemeny and Torch: " + str(time_out) + "s" + "\n")        print("Gamma value used for computing KT weights: " + str(gamma))        p.write("Gamma value used for computing KT weights: " + str(gamma) + "\n")        print("----Agreement errors with a random split----")        p.write("----Agreement errors with a random split----" + "\n")        print("Max KT Disagreement with this random split:", max_kt)        p.write("Max KT Disagreement with this random split: " + str(max_kt) + "\n")        data1 = [reviews[j] for j in reviewers[0]]        data2 = [reviews[j] for j in reviewers[1]]        scores1 = choix.opt_rankings(m, data1)        scores2 = choix.opt_rankings(m, data2)        mle_error, mle_disagreements, mle_ties = kendall_tau_scores(weights, scores1, scores2, args)        print("Disagreement with PL MLE:", mle_error, ("[" + str(mle_error / max_kt) + "]"))        p.write("Disagreement with PL MLE: " + str(mle_error) + (" [" + str(mle_error / max_kt) + "]") + "\n")        a_borda = torch.linspace(1.0, 0.0, steps=k)        kt_borda, borda_disagreements, borda_ties = kendall_tau(M, weights, a_borda, args, nabla)        print("Disagreement with Borda:", kt_borda, ("[" + str(kt_borda / max_kt) + "]"))        p.write("Disagreement with Borda: " + str(kt_borda) + (" [" + str(kt_borda / max_kt) + "]") + "\n")        kemeny_ranking_1, score_1 = kemeny_gurobi_lazy(data1, args, time_out=time_out, printout_mode=True)        kemeny_ranking_2, score_2 = kemeny_gurobi_lazy(data2, args, time_out=time_out, printout_mode=True)        kemeny_scores_1 = np.argsort(np.flip(kemeny_ranking_1))        kemeny_scores_2 = np.argsort(np.flip(kemeny_ranking_2))        kemeny_error, kemeny_disagreements, kemeny_ties = kendall_tau_scores(weights, kemeny_scores_1, kemeny_scores_2,                                                                             args)        print("Disagreement with Kemeny:", kemeny_error, ("[" + str(kemeny_error / max_kt) + "]"))        p.write("Disagreement with Kemeny: " + str(kemeny_error) + (" [" + str(kemeny_error / max_kt) + "]") + "\n")        a_torch, torch_losses = loss_sigmoid_torch(M, weights, args, 0, steep, sensitivity, min_steps, max_steps, lr=lr,                                                   print_mode=True, time_out=time_out)        kt_torch, torch_disagreements, torch_ties = kendall_tau(M, weights, a_torch, args, nabla)        print("Disagreement with Torch:", kt_torch, ("[" + str(kt_torch / max_kt) + "]"))        p.write("Disagreement with Torch: " + str(kt_torch) + (" [" + str(kt_torch / max_kt) + "]") + "\n")        print("Torch scores:", a_torch)        p.write("Torch scores:" + str(a_torch) + "\n")        proposal_ranks1 = [[] for _ in range(m)]  # for each proposal, we will add its ranks here. (REVIEWERS 1)        proposal_ranks2 = [[] for _ in range(m)]  # for each proposal, we will add its ranks here. (REVIEWERS 2)        for f in range(n):            if f in reviewers[0]:                for j in range(len(reviews[f])):                    proposal_ranks1[reviews[f][j]].append(j)            elif f in reviewers[1]:                for j in range(len(reviews[f])):                    proposal_ranks2[reviews[f][j]].append(j)        minmax_scores1 = ave_ranking_scores(proposal_ranks1, max_mix_rejection=True, rms_rejection=False)        minmax_scores2 = ave_ranking_scores(proposal_ranks2, max_mix_rejection=True, rms_rejection=False)        min_max_error, min_max_disagreements, min_max_ties = kendall_tau_scores(weights, minmax_scores1, minmax_scores2,                                                                                args)        print("Disagreement with Borda with Min/Max Rejection:", min_max_error,              ("[" + str(min_max_error / max_kt) + "]"))        p.write("Disagreement with Borda with Min/Max Rejection: " + str(min_max_error) + (                    " [" + str(min_max_error / max_kt) + "]") + "\n")        rms_scores1 = ave_ranking_scores(proposal_ranks1, max_mix_rejection=False, rms_rejection=True)        rms_scores2 = ave_ranking_scores(proposal_ranks2, max_mix_rejection=False, rms_rejection=True)        rms_error, rms_disagreements, rms_ties = kendall_tau_scores(weights, rms_scores1, rms_scores2, args)        print("Disagreement with RMS Rejection:", rms_error, ("[" + str(rms_error / max_kt) + "]"))        p.write("Disagreement with Borda with RMS Rejection: " + str(rms_error) + (                    " [" + str(rms_error / max_kt) + "]") + "\n")        print("----Output ranking when applied to the entire data----")        p.write("----Output ranking when applied to the entire data----" + "\n")        pl_mle_scores = choix.opt_rankings(m, reviews)        print_from_scores(pl_mle_scores, "PL MLE", p)        borda_scores = total_scores_all(M, a_borda, args)        print_from_scores(borda_scores, "Borda", p)        kemeny_output, score = kemeny_gurobi_lazy(reviews, args, time_out=time_out, printout_mode=True)        kemeny_scores = np.argsort(np.flip(kemeny_output))        print_from_scores(kemeny_scores, "Kemeny", p)        torch_scores = total_scores_all(M, a_torch, args)        print_from_scores(torch_scores, "Torch", p)        proposal_ranks_all = [[] for _ in range(m)]        for f in range(m):            proposal_ranks_all[f] = proposal_ranks1[f] + proposal_ranks2[f]        minmax_scores = ave_ranking_scores(proposal_ranks_all, max_mix_rejection=True, rms_rejection=False)        print_from_scores(minmax_scores, "Borda with Min/Max rejection", p)        rms_scores = ave_ranking_scores(proposal_ranks_all, max_mix_rejection=False, rms_rejection=True)        print_from_scores(rms_scores, "Borda with RMS rejection", p)        print("----Output KT distances of ranking producd from entire data----")        p.write("----Output KT distances of ranking producd from entire data----" + "\n")        weights = [1 for _ in range(m)]        scores = [pl_mle_scores, borda_scores, kemeny_scores, torch_scores, minmax_scores, rms_scores]        labels = ["PL MLE", "Borda", "Kemeny", "Torch", "Borda with Min/Max rejection", "Borda with RMS rejection"]        z = len(scores)        for i in range(z):            for j in range(i + 1, z):                error, disagreements, ties = kendall_tau_scores(weights, scores[i], scores[j], args, punish_ties=False)                print("KT for " + labels[i] + " vs. " + labels[j] + ":", error)                p.write("KT for " + labels[i] + " vs. " + labels[j] + ": " + str(error) + "\n")    # In[ ]:    import itertools    n_trials = 500    with open('ave_results_10.txt', 'w') as p:        # to keep track of average disagreement scores        ave_pl_mle_error = 0        ave_borda_error = 0        ave_minmax_error = 0        ave_rms_error = 0        ls_pl_mle_error = []        ls_borda_error = []        ls_minmax_error = []        ls_rms_error = []        all_max_kt = []        # to keep track of which ranking of scores appeared how many times        all_perms = list(itertools.permutations(range(4)))        count_perm = {}        for perm in all_perms:            count_perm[perm] = 0        for _ in tqdm(range(n_trials)):            args = (n, m, k, l)            gamma = 2            reviewers, M, weights = reviewer_split(reviews, args, gamma)            max_kt = 0  # The maximum weighted KT possible for this specific split            for i in range(n):                for j in range(i + 1, n):                    max_kt += weights[i] * weights[j]            all_max_kt.append(max_kt)            data1 = [reviews[j] for j in reviewers[0]]            data2 = [reviews[j] for j in reviewers[1]]            scores1 = choix.opt_rankings(m, data1)            scores2 = choix.opt_rankings(m, data2)            mle_error, mle_disagreements, mle_ties = kendall_tau_scores(weights, scores1, scores2, args)            ave_pl_mle_error += mle_error / max_kt            ls_pl_mle_error.append(mle_error / max_kt)            a_borda = torch.linspace(1.0, 0.0, steps=k)            kt_borda, borda_disagreements, borda_ties = kendall_tau(M, weights, a_borda, args, nabla)            ave_borda_error += kt_borda / max_kt            ls_borda_error.append(kt_borda / max_kt)            proposal_ranks1 = [[] for _ in range(m)]  # for each proposal, we will add its ranks here. (REVIEWERS 1)            proposal_ranks2 = [[] for _ in range(m)]  # for each proposal, we will add its ranks here. (REVIEWERS 2)            for f in range(n):                if f in reviewers[0]:                    for j in range(len(reviews[f])):                        proposal_ranks1[reviews[f][j]].append(j)                elif f in reviewers[1]:                    for j in range(len(reviews[f])):                        proposal_ranks2[reviews[f][j]].append(j)            minmax_scores1 = ave_ranking_scores(proposal_ranks1, max_mix_rejection=True, rms_rejection=False)            minmax_scores2 = ave_ranking_scores(proposal_ranks2, max_mix_rejection=True, rms_rejection=False)            min_max_error, min_max_disagreements, min_max_ties = kendall_tau_scores(weights, minmax_scores1,                                                                                    minmax_scores2, args)            ave_minmax_error += min_max_error / max_kt            ls_minmax_error.append(min_max_error / max_kt)            rms_scores1 = ave_ranking_scores(proposal_ranks1, max_mix_rejection=False, rms_rejection=True)            rms_scores2 = ave_ranking_scores(proposal_ranks2, max_mix_rejection=False, rms_rejection=True)            rms_error, rms_disagreements, rms_ties = kendall_tau_scores(weights, rms_scores1, rms_scores2, args)            ave_rms_error += rms_error / max_kt            ls_rms_error.append(rms_error / max_kt)            count_perm[tuple(np.argsort([mle_error, kt_borda, min_max_error, rms_error]))] += 1        ave_pl_mle_error /= n_trials        ave_borda_error /= n_trials        ave_minmax_error /= n_trials        ave_rms_error /= n_trials        print("----Agreement errors with a random split----")        p.write("----Agreement errors with a random split----" + "\n")        print("Disagreement with PL MLE:", ave_pl_mle_error)        p.write("Disagreement with PL MLE: " + str(ave_pl_mle_error) + "\n")        print("Disagreement with Borda:", ave_borda_error)        p.write("Disagreement with Borda: " + str(ave_borda_error) + "\n")        print("Disagreement with Borda with Min/Max Rejection:", ave_minmax_error)        p.write("Disagreement with Borda with Min/Max Rejection: " + str(ave_minmax_error) + "\n")        print("Disagreement with RMS Rejection:", ave_rms_error)        p.write("Disagreement with Borda with RMS Rejection: " + str(ave_rms_error) + "\n")    # In[ ]:    with open('torch_results_10.txt', 'w') as p:        counter = 1        start = time.time()        while time.time() - start < 13 * 60 * 60:            args = (n, m, k, l)            gamma = 2            reviewers, M, weights = reviewer_split(reviews, args, gamma)            max_kt = 0  # The maximum weighted KT possible for this specific split            for i in range(n):                for j in range(i + 1, n):                    max_kt += weights[i] * weights[j]            a_torch, torch_losses = loss_sigmoid_torch(M, weights, args, 0, steep, sensitivity, min_steps, max_steps,                                                       lr=lr, print_mode=True)            kt_torch, torch_disagreements, torch_ties = kendall_tau(M, weights, a_torch, args, nabla)            print("Trial", counter, "Torch scores:", a_torch)            p.write("Trial " + str(counter) + " Torch scores:" + str(a_torch) + "\n")            print("Trial", counter, "Disagreement with Torch:", kt_torch, ("[" + str(kt_torch / max_kt) + "]"))            p.write("Trial " + str(counter) + " Disagreement with Torch: " + str(kt_torch) + (                        " [" + str(kt_torch / max_kt) + "]") + "\n")            counter += 1